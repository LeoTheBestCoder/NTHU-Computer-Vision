{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0MrEVoVmhOy"
      },
      "source": [
        "# Computer Vision Homework 3: Big vs Small Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0hnrUlYrGWS"
      },
      "source": [
        "## Brief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_zuWZp5rSyN"
      },
      "source": [
        "Due date: Nov 16, 2022\n",
        "\n",
        "Required files: `homework-3.ipynb`, `report.pdf`\n",
        "\n",
        "To download the jupyter notebook from colab, you can refer to the colab tutorial we gave.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7423NauKQ6"
      },
      "source": [
        "## Codes for Problem 1 and Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX6pBqvV6RCq"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73wanLwflUdb"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "from torchvision import transforms, models, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtXEq_Yx5j-L"
      },
      "source": [
        "### Check GPU Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz3wOsYwmEz8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbpaGDdwnX9g"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAoPtdOR5ojk"
      },
      "source": [
        "### Set the Seed to Reproduce the Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wphy638XBNj-"
      },
      "outputs": [],
      "source": [
        "def set_all_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_all_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmcH3NAH4wq"
      },
      "source": [
        "### Create Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VHp_O3_JgZE"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Pad(4, padding_mode='reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transform)\n",
        "valid_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "sixteenth_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//16)\n",
        "half_train_sampler = RandomSampler(train_dataset, num_samples=len(train_dataset)//2)\n",
        "\n",
        "sixteenth_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sixteenth_train_sampler)\n",
        "half_train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=half_train_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFDtcWRnFS9"
      },
      "source": [
        "### Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgZV0CodnFS9"
      },
      "outputs": [],
      "source": [
        "# HINT: Remember to change the model to 'resnet50' and the weights to weights=\"IMAGENET1K_V1\" when needed.\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None)\n",
        "\n",
        "# Background: The original resnet18 is designed for ImageNet dataset to predict 1000 classes.\n",
        "# TODO: Change the output of the model to 10 class.\n",
        "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZo50knhnFS_"
      },
      "source": [
        "### Training and Testing Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlXKJeYWnFTA"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in the code cell according to the pytorch tutorial we gave.\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for X, y in tqdm(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagatopn\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pred = pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    \n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "    return avg_epoch_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    num_batches = len(dataloader)\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            epoch_loss += loss_fn(pred, y).item()\n",
        "            pred = pred.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    \n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    avg_acc = correct / size\n",
        "\n",
        "    return avg_epoch_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 2\n",
        "train_acc_list = []\n",
        "train_loss_list = []\n",
        "test_acc_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "big_model_acc = np.zeros(3)\n",
        "small_model_acc = np.zeros(3)\n",
        "\n",
        "x = np.arange(epochs)\n",
        "for m in ['ResNet18', 'ResNet50']:\n",
        "    # Choose model\n",
        "    if m == 'ResNet18':\n",
        "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)\n",
        "        model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "        model = model.to(device)\n",
        "    elif m == 'ResNet50':\n",
        "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None)\n",
        "        model.fc = nn.Linear(in_features=2048, out_features=10, bias=True)\n",
        "        model = model.to(device)\n",
        "    \n",
        "    # Choose data loader\n",
        "    for datasize in ['all', 'half', '1/16']:\n",
        "        if datasize == 'all':\n",
        "            dataloader = train_dataloader\n",
        "        elif datasize == 'half':\n",
        "            dataloader = half_train_dataloader\n",
        "        elif datasize == '1/16':\n",
        "            dataloader = sixteenth_train_dataloader\n",
        "\n",
        "        train_acc_list = []\n",
        "        train_loss_list = []\n",
        "        test_acc_list = []\n",
        "        test_loss_list = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
        "            test_loss, test_acc = test(valid_dataloader, model, loss_fn)\n",
        "            print(f\"Epoch {epoch + 1:2d}: Loss = {train_loss:.4f} Acc = {train_acc:.2f} Test_Loss = {test_loss:.4f} Test_Acc = {test_acc:.2f}\")\n",
        "            train_acc_list.append(train_acc)\n",
        "            train_loss_list.append(train_loss)\n",
        "            test_acc_list.append(test_acc)\n",
        "            test_loss_list.append(test_loss)\n",
        "        \n",
        "        if m == 'ResNet18':\n",
        "            if datasize == 'all':\n",
        "                small_model_acc[2] = test_acc\n",
        "            elif datasize == 'half':\n",
        "                small_model_acc[1] = test_acc\n",
        "            elif datasize == '1/16':\n",
        "                small_model_acc[0] = test_acc\n",
        "            \n",
        "        elif m == 'ResNet50':\n",
        "            if datasize == 'all':\n",
        "                big_model_acc[2] = test_acc\n",
        "            elif datasize == 'half':\n",
        "                big_model_acc[1] = test_acc\n",
        "            elif datasize == '1/16':\n",
        "                big_model_acc[0] = test_acc\n",
        "            \n",
        "\n",
        "        plt.plot(x, train_acc_list, color='red', label='train acc')\n",
        "        plt.plot(x, test_acc_list, '--', color='blue', label='test acc')\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.title(f'Accuracy (Train {m} on {datasize} data)')\n",
        "        plt.savefig(f'fig/Accuracy (Train {m} on {datasize} data).png')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(x, train_loss_list, color='red', label='train loss')\n",
        "        plt.plot(x, test_loss_list, '--', color='blue', label='test loss')\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title(f'Loss (Train {m} on {datasize} data)')\n",
        "        plt.savefig(f'fig/Loss (Train {m} on {datasize} data).png')\n",
        "        plt.show()\n",
        "\n",
        "plt.plot([1/16, 0.5, 1], big_model_acc, color='red', label='Big model')\n",
        "plt.plot([1/16, 0.5, 1], big_model_acc, color='green', label='Small model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('Dataset size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Dataset size vs accuracy')\n",
        "plt.savefig('result.png')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqBGAUm6b5W"
      },
      "source": [
        "## Codes for Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SBFMzPT6cP4"
      },
      "outputs": [],
      "source": [
        "# TODO: Try to achieve the best performance given all training data using whatever model and training strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTSafuelpRYJ"
      },
      "source": [
        "## Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZctBdkurpQS"
      },
      "source": [
        "1. (30%) Finish the rest of the codes for Problem 1 and Problem 2 according to the hint. (2 code cells in total.)\n",
        "2. Train small model (resnet18) and big model (resnet50) from scratch on `sixteenth_train_dataloader`, `half_train_dataloader`, and `train_dataloader` respectively.\n",
        "3. (30%) Achieve the best performance given all training data using whatever model and training strategy.  \n",
        "  (You cannot use the model that was pretrained on CIFAR10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786fQTdk0msC"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsd2yTmB0k5t"
      },
      "source": [
        "\n",
        "- (30%) The relationship between the accuracy, model size, and the training dataset size.  \n",
        "    (Total 6 models. Small model trains on the sixteenth, half, and all data. Big model trains on the sixteenth, half, and all data.)\n",
        "- (10%) What if we train the ResNet with ImageNet initialized weights (`weights=\"IMAGENET1K_V1\"`), how would the relationship change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWDxF-xIueMM"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXpmSj2ufkh"
      },
      "source": [
        "1. [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MemcOLK_4ULJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('cvhw3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f441a40f0ee66cb9bb6aa8e46d424e2bbf99fd41e9a43bc42947592a9029ae9e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
